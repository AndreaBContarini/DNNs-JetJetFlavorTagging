# -*- coding: utf-8 -*-
"""Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14HkVuayZGPtaUGO26eUvC3HefEQc_tJL

#Prima Parte: Conversione del file Json originale in un file npz

Questa procedura viene effettuata per poter utilizzare più dati nella rete, essendo la funzione per caricare file numpy compressi meno dispendiosa in termini di RAM.

Installazione delle librerie utili al trattamento dei dati, e oltre.
"""

!pip install pandas

!pip install torch

!pip install torchvision

"""Import delle Librerie"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torchvision
from torchvision.transforms import ToTensor
from torch.utils.data import DataLoader, Dataset
from torch import nn
import torch.nn.functional as F
from torch import optim

"""Si scarica il dataset JSON dal sito specificato nell'articolo:"""

!wget http://mlphysics.ics.uci.edu/data/hb_jet_flavor_2016/dataset.json.gz

"""Estrazione del file"""

import gzip

 !gunzip dataset.json.gz

"""Si caricano 11 milioni di dati sul file Npz, numero che quasi eguaglia la grandezza del dataset originale.

Per scaricare il dataset numpy compresso si procede "Chunkwise" maneggiando pochi dati alla volta, in maniera tale che dopo ogni caricamento la RAM possa essere ripulita e riutilizzata. In questa maniera è possibile passare tutto su un numpy array.
"""

N = int(11E6) # Porzione ridotta del dataset
dim_chunk = int(1E4)
n_chunks = int(N/dim_chunk)
myfile = pd.DataFrame()

chunks = pd.read_json('dataset.json', lines=True, chunksize=dim_chunk, nrows = N) # Si carica il data-frame un chunk alla volta

features_names=['jet_pt', 'jet_eta', 't_2_d0_s', 't_3_d0_s', 't_2_z0_s', 't_3_z0_s', 'n_t_over_d0_thresh', 'jet_prob', 'jet_width_eta', 'jet_width_phi', 'vertex_s', 'n_secondary_vertices', 'n_secondary_vertex_t',
'delta_r_vertex', 'vertex_mass', 'vertex_energy_fraction']

columns_names = ['jet_pt', 'jet_eta', 't_2_d0_s', 't_3_d0_s', 't_2_z0_s', 't_3_z0_s', 'n_t_over_d0_thresh', 'jet_prob', 'jet_width_eta', 'jet_width_phi', 'vertex_s', 'n_secondary_vertices', 'n_secondary_vertex_t',
'delta_r_vertex', 'vertex_mass', 'vertex_energy_fraction', 'Label']

Dati = []
Targhette = []

for i in range(n_chunks):

  print("Chunk", i+1)

  dataset = next(iter(chunks))

  data = pd.concat([dataset[0], dataset[1], dataset[3].apply(pd.Series), dataset[4].apply(pd.Series), dataset[2]], axis = 1).set_axis(columns_names, axis=1) # "set_axis" permette di rinominare le colonne (axis=1)

  data = data.replace('inf',-1.0)
  data = data.replace('-inf',-1.0)
  data = data.replace('-NaN',-1.0)
  data = data.replace('NaN',-1.0)
  data['Label'] = data['Label'].replace(4, 0)
  data['Label'] = data['Label'].replace(5, 1)

  y = data['Label'].to_numpy()
  X = data[features_names].to_numpy()

  Dati.append(X)
  Targhette.append(y)

Dati=np.array(Dati)
Targhette=np.array(Targhette)

"""Si salva preliminarmente il dataset diviso in chunk"""

np.savez('Dataset_numpy', X=Dati, y=Targhette)

"""Ricaricandolo ci si accorge che esso non ha la shape voluta per un eventuale allenamento delle reti neurali (infatti i vettori di feature hanno dimensione 16)."""

myfile = np.load('Dataset_numpy.npz')
Data = myfile['X']
Labels = myfile ['y']

print(Data.shape)
print(Labels.shape)

"""Per questo motivo si fa un reshape dei dati."""

Data = np.reshape(Data,(1100*10000,16))
Labels = np.reshape(Labels, 1100*10000)
print(Data.shape)
print(Labels.shape)

"""E infine si salva un file numpy compresso facilmente leggibile da colab con poco utilizzo di RAM, con la giusta shape per lavorare"""

np.savez('Dataset_numpy', X=Dati, y=Labels)

"""#Seconda parte: Analisi del dataset completo

Si carica il dataset previamente creato su Colab
"""

!rm -rf Dataset_numpy.npz
!gdown 19Qu3CfGdsaV70U-WMTRQS3GGCH-lN7SR

"""In questo caso questo stralcetto di codice è ridondante andando a lavorare con tutto il dataset, ma si è preferito scriverlo in questo modo in modo tale da variare il numero N di dati da leggere dal file."""

N = int(11E6)
import numpy as np
import matplotlib.pyplot as plt

dataset = np.load('Dataset_numpy.npz')
tmp_X = dataset['X'].reshape(int(11E6),16)
tmp_y = dataset['y'].reshape(int(11E6),1)
X = tmp_X[:N]
y = tmp_y[:N]

print(np.shape(y))
print(np.shape(X))

"""Nella generazione del file npz si è passati da una classificazione ternaria ad una classificazione binaria, in quanto la task richiesta era quella di discernimento del segnale dal fondo.

Nella cella di sotto si conta il numero di istanze in cui è presente una determinata label.
"""

print('Il numero di Label "1" è pari a:',np.count_nonzero(y))
print('Mentre il numero di Label "0" è pari a:', np.count_nonzero(y-1))

n,bins,patches = plt.hist(y, bins=4, density=False)
plt.title('Istogramma occorrenza delle Label')
plt.ylabel('Occorrenze')
plt.tight_layout()
set_axis_off()
plt.show()

"""Creiamo ora ai fini della nostra analisi due Numpy array contenenti uno le feature riguardanti la label 0 e uno quelle riguardanti la label 1."""

X_0=np.array([np.append(X[i],0) for i in range(len(y)) if y[i]==0])
X_1=np.array([np.append(X[i],1) for i in range(len(y)) if y[i]==1])

print('X_0 shape:', X_0.shape)
print('X_1 shape:', X_1.shape)

"""Studiamo ora le correlazioni, labelwise e sul dataset completo."""

#CORRELAZIONI LABEL 0
#features_names=['jet_pt', 'jet_eta', 't_2_d0_s', 't_3_d0_s', 't_2_z0_s', 't_3_z0_s', 'n_t_over_d0_thresh', 'jet_prob', 'jet_width_eta', 'jet_width_phi', 'vertex_s', 'n_secondary_vertices', 'n_secondary_vertex_t', 'delta_r_vertex', 'vertex_mass', 'vertex_energy_fraction']

features_names=['jet p$_t$', 'jet $\eta$', 'track 2 d$_0$ signif.', 'track 3 d$_0$ signif.',
                'track 2 z$_0$ signif.', 'track 3 d$_0$ signif.', '#tracks over d$_0$ trsh.', 'jet prob.',
                'jet width $\eta$', 'jet width $\phi$', 'vertex signif.', 'number of SV', 'number of SV tracks',
                '$\Delta$R to vertex', 'vertex mass', 'vertex energy frac.']

nomi = features_names + ['Label']
correlazioni_0 = np.corrcoef(X_0[:,:16],rowvar=False)
correlazioni_1 = np.corrcoef(X_1[:,:16],rowvar=False)

plt.figure(figsize=(30,30))
plt.matshow(correlazioni_0,vmin=-1,vmax=1, cmap='hot_r')
plt.xticks(range(len(features_names)),features_names, fontsize=11, rotation=90)
plt.yticks(range(len(features_names)),features_names, fontsize=11)
bar = plt.colorbar(label = 'Correlation Index')

plt.tight_layout()
plt.savefig('Matrice_Correlazioni_Label_0.pdf',format='pdf', bbox_inches = "tight")
plt.show()

#CORRELAZIONI LABEL 1
plt.figure(figsize=(30,30))
plt.matshow(correlazioni_1,vmin=-1,vmax=1, cmap='hot_r')
plt.xticks(range(len(features_names)),features_names, fontsize=11, rotation=90)
plt.yticks(range(len(features_names)),features_names, fontsize=11)
bar = plt.colorbar(label = 'Correlation Index')

plt.tight_layout()
plt.savefig('Matrice_Correlazioni_Label_1.pdf',format='pdf', bbox_inches = "tight")
plt.show()

#CORRELAZIONI INTERO DATASET
correlazioni_tot = np.corrcoef(np.append(X,y,axis=1),rowvar=False)

plt.figure(figsize=(30,30))
plt.matshow(correlazioni_tot,vmin=-1,vmax=1, cmap='hot_r')
plt.xticks(range(len(nomi)),nomi, fontsize=11, rotation=90)
plt.yticks(range(len(nomi)),nomi, fontsize=11)
bar = plt.colorbar(label = 'Correlation Index')

plt.tight_layout()
plt.savefig('Matrice_Correlazioni_Tot.pdf',format='pdf', bbox_inches = "tight")
plt.show()

"""Per finire l'analisi preliminare, si genera l'istogramma delle feature divise per label, in modo da mostrare delle differenze in occorrenza legate alle features analizzate."""

features_names=['jet p$_t$ [Gev]', 'jet $\eta$', 'track 2 d$_0$ signif.', 'track 3 d$_0$ signif.',
                'track 2 z$_0$ signif.', 'track 3 d$_0$ signif.', 'number of tracks over d$_0$ treshold', 'jet prob.',
                'jet width $\eta$', 'jet width $\phi$', 'vertex signif.', 'number of SV', 'number of SV tracks',
                '$\Delta$R to vertex', 'vertex mass [Gev]', 'vertex energy fraction']

plt.figure(figsize=(20,20))
plt.title('Dataset')
for i in range(16):
  binnaggio = 50
  plt.subplot(4,4,i+1)
  if i in [0,1,6]:
    if i==6:
      binnaggio = 15
    n_0,bins_0,patches_0 = plt.hist(X_0[:,i], bins=binnaggio, density=True, color='orange', histtype='step',label='Label 0')
    n_1,bins_1,patches_1 = plt.hist(X_1[:,i], bins=binnaggio, density=True, color='purple', histtype='step',label='Label 1')
    plt.xlabel(features_names[i])
    plt.yscale('log')
    plt.legend()
    #plt.grid()
  else:
    if i==11:
      binnaggio = 12
    if i==12:
      binnaggio = 20
    X_0_partial = np.array([elem for elem in X_0[:,i] if elem!=-1.])
    X_1_partial = np.array([elem for elem in X_1[:,i] if elem!=-1.])
    n_0,bins_0,patches_0 = plt.hist(X_0_partial, bins=binnaggio, density=True, color='orange', histtype='step',label='Label 0')
    n_1,bins_1,patches_1 = plt.hist(X_1_partial, bins=binnaggio, density=True, color='purple', histtype='step',label='Label 1')
    plt.xlabel(features_names[i])
    plt.yscale('log')
    plt.legend()
    #plt.grid()
plt.tight_layout()
plt.savefig('Istogrammi_Features_Dataset.pdf',format='pdf')
plt.show()